# Experiment Scripts

This directory contains Makefiles and configuration for running grammar
learning experiments.  There are two pipelines:

- **`Makefile`** — the original NMF-based pipeline (kernel finding via
  distributional features + NMF, parameter estimation, Inside-Outside
  refinement, evaluation).
- **`Makefile.neural`** — the neural pipeline (RNN-based cloze models,
  antichain anchor selection, Renyi parameter estimation, SGD
  refinement, evaluation).

## Directory layout

Each experiment operates on a base directory containing one
subdirectory per grammar.  The grammar directories are named `g000`,
`g001`, etc.  The only required input file is `grammar.pcfg`; everything
else is generated by the pipeline.

```
basedir/
├── g000/
│   ├── grammar.pcfg              # input: target PCFG
│   ├── trees                     # sampled derivation trees (generated)
│   ├── corpus.txt                # string yields extracted from trees (generated)
│   ├── ml.pcfg                   # maximum-likelihood PCFG from trees (generated)
│   ├── rnn_cloze.pt              # trained RNN single model (generated)
│   ├── rnn_gap_cloze.pt          # trained RNN gap model (generated)
│   ├── pos_cloze_k2.pt           # trained positional single model (generated)
│   ├── pos_pair_cloze_k2.pt      # trained positional pair model (generated)
│   ├── neural_init.pcfg          # initial PCFG from Renyi estimation (generated)
│   ├── neural_sgd.pcfg           # PCFG after SGD refinement (generated)
│   ├── neural_pipeline_results.json  # evaluation results (generated)
│   ├── neural_pipeline.log       # pipeline stdout/stderr (generated)
│   └── ...
├── g001/
│   └── ...
└── neural_pipeline_summary.json  # cross-grammar summary (generated)
```

### Grammar format (.pcfg)

One production per line: `probability LHS -> RHS...`

```
8.426967e-02 NT1 -> NT3 NT1
1.163012e-02 NT1 -> NT5 qab
```

### Tree format (trees)

S-expression derivation trees, one per line:

```
(S (NT1 (NT3 qab) (NT1 (NT6 rhx) (NT7 nhl))) (NT2 apw))
```

### Corpus format (.txt)

String yields (one sentence per line, space-separated tokens),
extracted from the trees file:

```
qab rhx nhl apw
uuu zal psw
```

## Quick start (neural pipeline)

### Prerequisites

1. Python with numpy, scipy, matplotlib, torch.
2. The C Inside-Outside binary (for SGD refinement):
   ```
   cd ../lib && make io
   ```
3. Grammar files in the base directory.  To generate test grammars
   from scratch, see `setup.sh` or the
   [testpcfg](https://github.com/alexc17/testpcfg) repository.

### Running

All commands are run from this `scripts/` directory.

```bash
# Full pipeline: corpus sampling -> model training -> kernel finding
#                -> parameter estimation -> SGD -> evaluation
# Runs only on grammars where the discovered NT count matches the target.
make -f Makefile.neural -j 10 all

# Individual stages:
make -f Makefile.neural -j 10 corpus       # sample corpora from grammars
make -f Makefile.neural -j 10 rnn_models   # train RNN cloze models
make -f Makefile.neural -j 10 pos_models   # train positional cloze models
make -f Makefile.neural -j 10 models       # train both model types
make -f Makefile.neural -j 10 kernels      # anchor discovery only (fast)
```

Dependencies are tracked automatically: running `all` will sample
corpora and train models as needed.

### Cleanup

```bash
make -f Makefile.neural clean          # remove pipeline outputs (results, logs, PCFGs)
make -f Makefile.neural clean_models   # remove trained models
make -f Makefile.neural clean_corpus   # remove sampled corpora
make -f Makefile.neural clean_all      # remove everything generated
```

## Configuration

All experiment parameters live in `config.json`.  The Makefile and
Python scripts both read from it; the Makefile contains no parameter
definitions of its own.

```json
{
    "basedir": "../tmp/jm_experiment",

    "corpus": {
        "n_sentences": 1000000,
        "seed": 1
    },

    "training": {
        "n_epochs": 10,
        "embedding_dim": 64,
        "hidden_dim": 128,
        "batch_size": 4096,
        "seed": 42
    },

    "pipeline": {
        "model_type": "rnn",
        "pos_k": 2,
        "ml_maxlength": 10,
        "maxlength": 15,
        "maxcount": 1000,
        "stepsize": 0.5,
        "eval_corpus_size": 100000,
        "max_terminals": 300,
        "epsilon": 1.5,
        "pipeline_epochs": 3
    }
}
```

| Section    | Key               | Description                                    |
|------------|-------------------|------------------------------------------------|
| `corpus`   | `n_sentences`     | Number of sentences to sample from grammar     |
|            | `seed`            | Random seed for corpus sampling                |
| `training` | `n_epochs`        | Training epochs for cloze models               |
|            | `embedding_dim`   | Embedding dimension for neural models          |
|            | `hidden_dim`      | Hidden layer dimension                         |
|            | `batch_size`      | Training batch size                            |
|            | `seed`            | Random seed for model training                 |
| `pipeline` | `model_type`      | Neural model type: `rnn`, `positional`, `bow`  |
|            | `pos_k`           | Context window width for positional/bow models |
|            | `ml_maxlength`    | Max sentence length for ML PCFG from trees     |
|            | `maxlength`       | Max sentence length for Inside-Outside SGD     |
|            | `maxcount`        | Mini-batch size for SGD (sentences)            |
|            | `stepsize`        | SGD learning rate                              |
|            | `eval_corpus_size`| Corpus subset size for SGD training            |
|            | `max_terminals`   | Max candidate terminals for anchor selection   |
|            | `epsilon`         | Log-ratio tolerance for subset test            |
|            | `pipeline_epochs` | Epochs for loading cached RNN in pipeline      |

To run with a different configuration, point to a different file:

```bash
make -f Makefile.neural config=config_large.json -j 10 all
```

Individual parameters can also be overridden on the command line when
running scripts directly:

```bash
python ../locallearner/run_neural_pipeline.py --config config.json --base /other/dir g007
python ../locallearner/train_cloze_models.py gdir --config config.json --n_epochs 20
```

## Pipeline stages in detail

### 1. Corpus sampling

Samples `n_sentences` derivation trees from `grammar.pcfg`, storing
full trees in `trees`.  Two files are then derived:

- `corpus.txt` — the string yields (for model training and SGD).
- `ml.pcfg` — the maximum-likelihood PCFG estimated from the trees
  (sentences longer than `ml_maxlength` are excluded).  This serves
  as an oracle baseline for evaluation.

### 2. Cloze model training

Trains neural cloze models P(word | context) on the corpus.  Two
architectures are supported:

- **RNN** (`rnn_cloze.pt`, `rnn_gap_cloze.pt`): Bidirectional GRU
  over variable-length left and right contexts.  The gap model is a
  variant trained with a one-word gap for pair prediction.
- **Positional** (`pos_cloze_k2.pt`, `pos_pair_cloze_k2.pt`):
  Fixed-width context window of `k` words on each side.

Training is skipped if the model files already exist.  Use `--force`
to retrain.

### 3. Anchor discovery

Uses the trained single cloze model to find anchor words (one per
nonterminal) via an antichain algorithm:

1. Compute P(word | context) for each candidate terminal.
2. For each pair of candidates, test whether one's context
   distribution is a subset of the other's using a quantile-based
   violation fraction test.
3. Build an antichain (set of incomparable elements) by processing
   candidates in decreasing frequency order.
4. The antichain members become the anchors.

The noise threshold is self-calibrated from the data using a
frequency-adaptive model.

### 4. Parameter estimation

If the discovered number of nonterminals matches the target grammar:

1. Train the pair/gap cloze model (if not cached).
2. Estimate lexical parameters (P(word | NT)) using the single model.
3. Estimate binary rule parameters (P(NT1 NT2 | NT)) using Renyi
   divergence with the pair model.
4. Build and normalize a WCFG, then convert to a proper PCFG.

### 5. SGD refinement

One epoch of stochastic gradient descent using the C Inside-Outside
binary, with linear interpolation updates.

### 6. Evaluation

All hypotheses (ML baseline, neural init, neural SGD) are evaluated
together in a single call so they share the same sampled trees.
Metrics computed:
- KL divergence (exact, smoothed)
- Labeled/unlabeled exact match
- Labeled/unlabeled micro-averaged Parseval F1
- Target ceiling (gold Viterbi)

Results are saved per-grammar as `neural_pipeline_results.json` with
separate entries for `ml`, `init`, `sgd`, and `target`.

## Original NMF pipeline

The original pipeline uses NMF-based kernel finding instead of neural
models.  It is driven by the plain `Makefile`:

```bash
make -j 50 json     # run full pipeline
make thin           # remove large intermediate files
make clean          # remove all generated files
```

This pipeline requires grammars in `../data/test*/` and the C IO
binary in `../bin/io`.  See `setup.sh` for initial setup.

## Running single grammars

The Python scripts can be run individually outside of Make:

```bash
# Sample trees
python ../locallearner/sample_corpus.py --omitprobs --seed 1 --n 1000000 \
    grammar.pcfg trees

# Extract yields
python ../locallearner/convert_trees_to_yields.py trees corpus.txt

# Build ML grammar from trees
python ../locallearner/convert_trees_to_pcfg.py --length 10 trees ml.pcfg

# Train models
python ../locallearner/train_cloze_models.py gdir/ --config config.json

# Full pipeline (single grammar)
python ../locallearner/run_neural_pipeline.py --config config.json g007

# Full pipeline (all grammars, sequential)
python ../locallearner/run_neural_pipeline.py --config config.json --all

# Kernels only
python ../locallearner/run_neural_pipeline.py --config config.json --kernels-only g007

# Visualize antichain decisions
python ../locallearner/plot_antichain.py gdir/ output.png --json decisions.json
```
